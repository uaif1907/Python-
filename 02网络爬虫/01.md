#### 1. 在 requests 模块中，requests.content 和 requests.text 什么区别
```
request.text返回的是Unicode型的数据。
request.content返回的是bytes型也就是二进制的数据。
```



#### 2. 简要写一下 lxml 模块的使用方法框架

```
使用lxml步骤：

1，引入包：from lxml import etree

2，解析获得的网页：html = etree.HTML(要解析的网页或文本)

3，使用etree匹配获得要取得的内容：

例如：获得 html中所有的li内容：

li = html.xpath('//li')

```



#### 3. 说一说 scrapy 的工作流程
```
调度器把requests-->引擎-->下载中间件--->下载器
下载器发送请求，获取响应---->下载中间件---->引擎--->爬虫中间件--->爬虫
爬虫提取url地址，组装成request对象---->爬虫中间件--->引擎--->调度器
爬虫提取数据--->引擎--->管道
管道进行数据的处理和保存
```
#### 4. scrapy 的去重原理
```
1.Scrapy本身自带有一个中间件;
2.scrapy源码中可以找到一个dupefilters.py去重器;
3.需要将dont_filter设置为False开启去重，默认是false去重，改为True,就是没有开启去重；
4 .对于每一个url的请求，调度器都会根据请求得相关信息加密得到一个指纹信息，并且将指纹信息和set()集合中的指纹信息进 行 比对，如果set()集合中已经存在这个数据，就不在将这个Request放入队列中;5.如果set()集合中没有存在这个加密后的数据，就将这个Request对象放入队列中，等待被调度。
```
#### 5. scrapy 中间件有几种类，你用过哪些中间件

```
scrapy的中间件理论上有三种(Schduler Middleware,Spider Middleware,Downloader Middleware),在应用上一般有以下两种

       1.爬虫中间件Spider Middleware
         主要功能是在爬虫运行过程中进行一些处理.

　　2.下载器中间件Downloader Middleware
         主要功能在请求到网页后,页面被下载时进行一些处理.
         
         
         
         
 1.Spider Middleware有以下几个函数被管理:
       - process_spider_input 接收一个response对象并处理,

         位置是Downloader-->process_spider_input-->Spiders(Downloader和Spiders是scrapy官方结构图中的组件)

       - process_spider_exception spider出现的异常时被调用

       - process_spider_output 当Spider处理response返回result时,该方法被调用

       - process_start_requests 当spider发出请求时,被调用

　　  位置是Spiders-->process_start_requests-->Scrapy Engine(Scrapy Engine是scrapy官方结构图中的组件)         

 　 2.Downloader Middleware有以下几个函数被管理
　　 - process_request  request通过下载中间件时，该方法被调用

　　 - process_response 下载结果经过中间件时被此方法处理

　　 - process_exception 下载过程中出现异常时被调用

      编写中间件时,需要思考要实现的功能最适合在那个过程处理,就编写哪个方法.

      中间件可以用来处理请求,处理结果或者结合信号协调一些方法的使用等.也可以在原有的爬虫上添加适应项目的其他功能,这一点在扩展中编写也可以达到目的,实际上扩展更加去耦合化,推荐使用扩展.
```



#### 6. 你写爬虫的时候都遇到过什么？反爬虫措施，你是怎么解决的？

```
添加请求头，使用代理，添加延迟，使用cookie,忽略ssl验证
```



#### 7. 为什么会用到代理？
```
爬虫会触发服务器的防爬机制，短时间内关闭用户的机器的ip连接，为了完成对目标的获取，则需要不同的ip来完成
```
#### 8. 代理失效了怎么处理？
```
https://baijiahao.baidu.com/s?id=1618299340903906830&wfr=spider&for=pc
1，将代理IP及其协议加载ProxyHandler赋给一个opener_support变量；2，将opener_support加载build_opener方法，创建opener；3，安装开瓶器。具体代码如下：从urllib导入请求def ProxySpider（网址，proxy_ip，标头）：opener_support = request.ProxyHandler（{'http'：proxy_ip}） 开瓶器= request.build_opener（opener_support） request.install_opener（开启） req = request.Request（URL，headers = header）rsp = request.urlopen（req）.read（）返回rsp

```
#### 9. 列出你知道 header 的内容以及信息
```
User-Agent：产生请求的浏览器类型

Accept：client端可识别的内容类型列表

Host：请求的主机名，允许多个域名同处一个ip地址，即虚拟主机
```
#### 10. 说一说打开浏览器访问 www.baidu.com 获取到结果，整个流程。

```
1， 依题主所假设的，在浏览器的地址栏输入http://www.baidu.com，这个时候，浏览器将会运行Hypertext Transfer Protocol（Hypertext Transfer Protocol,HTTP），这是一个应用层协议。

2， 通过域名系统（Domain Name System，DNS）,浏览器获得域名的IP地址（百度的IP：http://202.108.22.5/），这个域名系统也是运行在应用层上的。

3， 这个时候，浏览器已经产生一个HTTP包了，包头带着一个数据信息，表达的意思类似于在说：“度娘度娘，我要你的主页。”

4， 下图所示是浏览器与服务器之间的通信时，HTTP所起的作用（注意，这个时候通信还没开始，这个图只是为了说明HTTP所扮演的角色）

5， 浏览器将这个HTTP信息包交给TCP(传输控制协议)（ Transmission Control Protocol）,该协议运行于传输层。这是一个相邻的层之间进行交互的例子，较高层级（如HTTP）向较低层级（如TCP）表达服务需要，较低层级给较高层级提供服务。这种相邻层级的交互持续进行，直到抵达最底层。TCP的主要功能是与百度服务器的TCP程序进行连接并建立会话，就像在两者之间建立起一个管道那样，使两者之间的数据得以交互。

6， 直到这个时候，信息包仍然呆在你的电脑里面，没有发出去呢。这个时候，TCP将TCP信息包转发给IP层（Internet Protocol）。 这是个网络层协议。IP的重要功能是寻址和路由，找到一条通往百度的IP地址的最优数据通路。就像一个邮局把你的信件寄到收件人那样。同时，IP将自己的信息加到原有的数据包上，形成新的数据包。

7， 注意，直到这个时候，数据包还在你的电脑上呢，现在，数据包将由IP层交到网络接口层（network interface layer）。这个层定义了通过物理网络输出数据所需的协议与硬件要求。多数电脑使用以太网（Ethernet）。电脑这个时候将IP数据加上以太网帧头和帧尾打包成以太网帧。
8， 这个时候，电脑就可以传输以太网帧的物理数据了，使用真正的电信号走网线传输了。数据包也在这个时候真正离开电脑。当这些电信号到达百度的服务器时，这些电信号将被重新翻译成二进制数据。服务器将对这个数据包进行解析，过程与上述形成该数据包的过程相反。

```



#### 11. 爬取速度过快出现了验证码怎么处理
#### 12. scrapy 和 scrapy-redis 有什么区别？为什么选择 redis 数据库？
```
1) scrapy是一个Python爬虫框架，爬取效率极高，具有高度定制性，但是不支持分布式。而scrapy-redis一套基于redis数据库、运行在scrapy框架之上的组件，可以让scrapy支持分布式策略，Slaver端共享Master端redis数据库里的item队列、请求队列和请求指纹集合。

2) 为什么选择redis数据库，因为redis支持主从同步，而且数据都是缓存在内存中的，所以基于redis的分布式爬虫，对请求和数据的高频读取效率非常高。
```
#### 13. 分布式爬虫主要解决什么问题
```
1)ip

2)带宽

3）cpu

4）io



```
#### 14. 写爬虫是用多进程好？还是多线程好？为什么？
```
IO密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率(单线程下有IO操作会进行IO等待，造成不必要的时间浪费，而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自身机器的硬件情况，来设置多进程或多线程
```
#### 15. 
```
1 正则表达式:模糊匹配解析
2 html.parser:结构化解析
3 Beautiful Soup :结构化解析
4 lxml:结构化解析
```
#### 16. 需要登录的网页，如何解决同时限制 ip，cookie,session（其中有一些是动态生成的）在不使用动态爬取的情况下？
```
https://blog.csdn.net/lbjowen/article/details/82584951
```
#### 17. 验证码的解决（简单的：对图像做处理后可以得到的，困难的：验证码是点击，拖动等动态进行的？）

```
使用selenium进行模拟点击等操作
```



#### 18.请简要介绍下scrapy框架。
```
1、Scrapy Engine(引擎): 引擎负责控制数据流在系统的所有组件中流动，并在相应动作发生时触发事件。



2、Scheduler(调度器): 调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。



3、Downloader（下载器）： 下载器负责获取页面数据并提供给引擎，而后提供给spider。



4、Spider（爬虫）： Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。



5、Item Pipeline(管道)： Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存储到数据库中)。



6、Downloader Middlewares（下载中间件）： 下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。

7、Spider Middlewares（Spider中间件）： Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。


```

#### 19.为什么要使用scrapy框架？scrapy框架有哪些优点？
```
更容易构建大规模抓取项目；
异步处理请求的速度快
使用自动调节机制自动调整爬取速度
```

#### 20.scrapy框架有哪几个组件/模块？简单说一下工作流程。
```
Scrapy Engine 引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。

调度器(Scheduler) 调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。

下载器(Downloader) 下载器负责获取页面数据并提供给引擎，而后提供给spider。

Spiders Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 更多内容请看 Spiders 。

Item Pipeline Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。 更多内容查看 Item Pipeline 。

下载器中间件(Downloader middlewares) 下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response（也包括引擎传递给下载器的Request）。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 下载器中间件(Downloader Middleware) 。

一句话总结就是：处理下载请求部分

Spider中间件(Spider middlewares) Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 Spider中间件(Middleware) 。
一句话总结就是：处理解析部分

数据流(Data flow)
```

#### 21. scrapy如何实现分布式抓取？
```
scrapy-redis原理: 
1.spider解析下载器下载下来的response,返回item或者是links 
2.item或者links经过spidermiddleware的process_spider_out()方法，交给engine。 
3.engine将item交给itempipeline,将links交给调度器 
4.在调度器中，先将request对象利用scrapy内置的指纹函数，生成一个指纹对象 
5.如果request对象中的dont_filter参数设置为False,并且该request对象的指纹不在信息指纹的队列中，那么就把该request对象放到优先级的队列中 
6.从优先级队列中获取request对象，交给engine 
7.engine将request对象交给下载器下载，期间会通过downloadmiddleware的process_request()方法 
8.下载器完成下载，获得response对象，将该对象交给engine,期间会通过downloadmiddleware的process_response()方法 
9.engine将获得的response对象交给spider进行解析，期间会经过spidermiddleware的process_spider_input()方法 
10.从第一步开始循环

上面的十个步骤就是scrapy-redis的整体框架，与scrapy相差无几。本质的区别就是，将scrapy的内置的去重的队列和待抓取的request队列换成了redis的集合。就这一个小小的改动，就使得了scrapy-redis支持了分布式抓取。
```

#### 22.urlopen()

```
在 Python3 的 urllib 库中，所有和网络请求的相关方法都被集中到 urllib.request 模块下了。以下是 urlopen() 方法最基本的使用方法：

from urllib import request
resp = request.urlopen('https://www.baidu.com')
print(resp.read())
```

#### 23.urlretrieve()

```
这个方法可以方便的将网页上的一个文件保存到本地。以下代码可以非常方便的将百度的首页下载到本地：

from urllib import request
request.urlretrieve('http://www.baidu.com/', 'baidu.html')
```

#### 24.urlencode()

```
若 ulr 中包含了中文或其他特殊字符，则浏览器会自动给我们进行编码。要用爬虫模拟浏览器，就需要进行手动编码，此时就要用到 urlencode() 方法来实现。这个方法可以把字典数据转换为 url 编码的数据。例如：

from urllib import parse
 
url = 'https://www.baidu.com/s'
 
params = {"wd": "教学设计"}
qs = parse.urlencode(params)
url = url + "?ie=UTF-8&&" + qs
print(qs)
print(url)
resp = request.urlopen(url)
print(resp.read())
```

#### 25.parse_qs()

```
此方法可以将经过编码后的 ulr 参数进行解码。例如：

from urllib import parse
 
params = {'name': '木易', 'age': '21', 'sex': 'male'}
qs = parse.urlencode(params)
result = parse.parse_qs(qs)
print(result)
```

#### 26.urlparse() 和 urlsplit()

```
这两个方法用来对 url 的各个组成部分进行分割。最后可以一次性拿到全部的 url 组成部分，也可以单独拿出某个部分。例如：

from urllib import parse
 
url = 'https://baike.baidu.com/item/hello%20world/85501?fr=aladdin#2_13'
 
result = parse.urlparse(url)
print(result) //拿出全部
// 分别拿出各个部分
print('scheme:', result.scheme)
print('netloc:', result.netloc)
print('path:', result.path)
print('params:',result.params)  // urlsplit() 方法没有此项结果
print('query:', result.query)
print('fragment:', result.fragment)
```

