#### 1. 在 requests 模块中，requests.content 和 requests.text 什么区别
```
request.text返回的是Unicode型的数据。
request.content返回的是bytes型也就是二进制的数据。
```



#### 2. 简要写一下 lxml 模块的使用方法框架

```
使用lxml步骤：

1，引入包：from lxml import etree

2，解析获得的网页：html = etree.HTML(要解析的网页或文本)

3，使用etree匹配获得要取得的内容：

例如：获得 html中所有的li内容：

li = html.xpath('//li')

```



#### 3. 说一说 scrapy 的工作流程
```
调度器把requests-->引擎-->下载中间件--->下载器
下载器发送请求，获取响应---->下载中间件---->引擎--->爬虫中间件--->爬虫
爬虫提取url地址，组装成request对象---->爬虫中间件--->引擎--->调度器
爬虫提取数据--->引擎--->管道
管道进行数据的处理和保存
```
#### 4. scrapy 的去重原理
```
1.Scrapy本身自带有一个中间件;
2.scrapy源码中可以找到一个dupefilters.py去重器;
3.需要将dont_filter设置为False开启去重，默认是false去重，改为True,就是没有开启去重；
4 .对于每一个url的请求，调度器都会根据请求得相关信息加密得到一个指纹信息，并且将指纹信息和set()集合中的指纹信息进 行 比对，如果set()集合中已经存在这个数据，就不在将这个Request放入队列中;5.如果set()集合中没有存在这个加密后的数据，就将这个Request对象放入队列中，等待被调度。
```
#### 5. scrapy 中间件有几种类，你用过哪些中间件

```
scrapy的中间件理论上有三种(Schduler Middleware,Spider Middleware,Downloader Middleware),在应用上一般有以下两种

       1.爬虫中间件Spider Middleware
         主要功能是在爬虫运行过程中进行一些处理.

　　2.下载器中间件Downloader Middleware
         主要功能在请求到网页后,页面被下载时进行一些处理.
         
         
         
         
 1.Spider Middleware有以下几个函数被管理:
       - process_spider_input 接收一个response对象并处理,

         位置是Downloader-->process_spider_input-->Spiders(Downloader和Spiders是scrapy官方结构图中的组件)

       - process_spider_exception spider出现的异常时被调用

       - process_spider_output 当Spider处理response返回result时,该方法被调用

       - process_start_requests 当spider发出请求时,被调用

　　  位置是Spiders-->process_start_requests-->Scrapy Engine(Scrapy Engine是scrapy官方结构图中的组件)         

 　 2.Downloader Middleware有以下几个函数被管理
　　 - process_request  request通过下载中间件时，该方法被调用

　　 - process_response 下载结果经过中间件时被此方法处理

　　 - process_exception 下载过程中出现异常时被调用

      编写中间件时,需要思考要实现的功能最适合在那个过程处理,就编写哪个方法.

      中间件可以用来处理请求,处理结果或者结合信号协调一些方法的使用等.也可以在原有的爬虫上添加适应项目的其他功能,这一点在扩展中编写也可以达到目的,实际上扩展更加去耦合化,推荐使用扩展.
```



#### 6. 你写爬虫的时候都遇到过什么？反爬虫措施，你是怎么解决的？

```
添加请求头，使用代理，添加延迟，使用cookie,忽略ssl验证
```



#### 7. 为什么会用到代理？
```
爬虫会触发服务器的防爬机制，短时间内关闭用户的机器的ip连接，为了完成对目标的获取，则需要不同的ip来完成
```
#### 8. 代理失效了怎么处理？
```
https://baijiahao.baidu.com/s?id=1618299340903906830&wfr=spider&for=pc
1，将代理IP及其协议加载ProxyHandler赋给一个opener_support变量；2，将opener_support加载build_opener方法，创建opener；3，安装开瓶器。具体代码如下：从urllib导入请求def ProxySpider（网址，proxy_ip，标头）：opener_support = request.ProxyHandler（{'http'：proxy_ip}） 开瓶器= request.build_opener（opener_support） request.install_opener（开启） req = request.Request（URL，headers = header）rsp = request.urlopen（req）.read（）返回rsp

```
#### 9. 列出你知道 header 的内容以及信息
```
User-Agent：产生请求的浏览器类型

Accept：client端可识别的内容类型列表

Host：请求的主机名，允许多个域名同处一个ip地址，即虚拟主机
```
#### 10. 说一说打开浏览器访问 www.baidu.com 获取到结果，整个流程。

```
1， 依题主所假设的，在浏览器的地址栏输入http://www.baidu.com，这个时候，浏览器将会运行Hypertext Transfer Protocol（Hypertext Transfer Protocol,HTTP），这是一个应用层协议。

2， 通过域名系统（Domain Name System，DNS）,浏览器获得域名的IP地址（百度的IP：http://202.108.22.5/），这个域名系统也是运行在应用层上的。

3， 这个时候，浏览器已经产生一个HTTP包了，包头带着一个数据信息，表达的意思类似于在说：“度娘度娘，我要你的主页。”

4， 下图所示是浏览器与服务器之间的通信时，HTTP所起的作用（注意，这个时候通信还没开始，这个图只是为了说明HTTP所扮演的角色）

5， 浏览器将这个HTTP信息包交给TCP(传输控制协议)（ Transmission Control Protocol）,该协议运行于传输层。这是一个相邻的层之间进行交互的例子，较高层级（如HTTP）向较低层级（如TCP）表达服务需要，较低层级给较高层级提供服务。这种相邻层级的交互持续进行，直到抵达最底层。TCP的主要功能是与百度服务器的TCP程序进行连接并建立会话，就像在两者之间建立起一个管道那样，使两者之间的数据得以交互。

6， 直到这个时候，信息包仍然呆在你的电脑里面，没有发出去呢。这个时候，TCP将TCP信息包转发给IP层（Internet Protocol）。 这是个网络层协议。IP的重要功能是寻址和路由，找到一条通往百度的IP地址的最优数据通路。就像一个邮局把你的信件寄到收件人那样。同时，IP将自己的信息加到原有的数据包上，形成新的数据包。

7， 注意，直到这个时候，数据包还在你的电脑上呢，现在，数据包将由IP层交到网络接口层（network interface layer）。这个层定义了通过物理网络输出数据所需的协议与硬件要求。多数电脑使用以太网（Ethernet）。电脑这个时候将IP数据加上以太网帧头和帧尾打包成以太网帧。
8， 这个时候，电脑就可以传输以太网帧的物理数据了，使用真正的电信号走网线传输了。数据包也在这个时候真正离开电脑。当这些电信号到达百度的服务器时，这些电信号将被重新翻译成二进制数据。服务器将对这个数据包进行解析，过程与上述形成该数据包的过程相反。

```



#### 11. 爬取速度过快出现了验证码怎么处理
#### 12. scrapy 和 scrapy-redis 有什么区别？为什么选择 redis 数据库？
```
1) scrapy是一个Python爬虫框架，爬取效率极高，具有高度定制性，但是不支持分布式。而scrapy-redis一套基于redis数据库、运行在scrapy框架之上的组件，可以让scrapy支持分布式策略，Slaver端共享Master端redis数据库里的item队列、请求队列和请求指纹集合。

2) 为什么选择redis数据库，因为redis支持主从同步，而且数据都是缓存在内存中的，所以基于redis的分布式爬虫，对请求和数据的高频读取效率非常高。
```
#### 13. 分布式爬虫主要解决什么问题
```
1)ip

2)带宽

3）cpu

4）io



```
#### 14. 写爬虫是用多进程好？还是多线程好？为什么？
```
IO密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率(单线程下有IO操作会进行IO等待，造成不必要的时间浪费，而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自身机器的硬件情况，来设置多进程或多线程
```
#### 15. 
```
1 正则表达式:模糊匹配解析
2 html.parser:结构化解析
3 Beautiful Soup :结构化解析
4 lxml:结构化解析
```
#### 16. 需要登录的网页，如何解决同时限制 ip，cookie,session（其中有一些是动态生成的）在不使用动态爬取的情况下？
```
https://blog.csdn.net/lbjowen/article/details/82584951
```
#### 17. 验证码的解决（简单的：对图像做处理后可以得到的，困难的：验证码是点击，拖动等动态进行的？）

```
使用selenium进行模拟点击等操作
```



#### 18.请简要介绍下scrapy框架。
```
1、Scrapy Engine(引擎): 引擎负责控制数据流在系统的所有组件中流动，并在相应动作发生时触发事件。



2、Scheduler(调度器): 调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。



3、Downloader（下载器）： 下载器负责获取页面数据并提供给引擎，而后提供给spider。



4、Spider（爬虫）： Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。



5、Item Pipeline(管道)： Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存储到数据库中)。



6、Downloader Middlewares（下载中间件）： 下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。

7、Spider Middlewares（Spider中间件）： Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。


```

#### 19.为什么要使用scrapy框架？scrapy框架有哪些优点？
```
更容易构建大规模抓取项目；
异步处理请求的速度快
使用自动调节机制自动调整爬取速度
```

#### 20.scrapy框架有哪几个组件/模块？简单说一下工作流程。
```
Scrapy Engine 引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。

调度器(Scheduler) 调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。

下载器(Downloader) 下载器负责获取页面数据并提供给引擎，而后提供给spider。

Spiders Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 更多内容请看 Spiders 。

Item Pipeline Item Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。 更多内容查看 Item Pipeline 。

下载器中间件(Downloader middlewares) 下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response（也包括引擎传递给下载器的Request）。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 下载器中间件(Downloader Middleware) 。

一句话总结就是：处理下载请求部分

Spider中间件(Spider middlewares) Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 Spider中间件(Middleware) 。
一句话总结就是：处理解析部分

数据流(Data flow)
```

#### 21. scrapy如何实现分布式抓取？
```
scrapy-redis原理: 
1.spider解析下载器下载下来的response,返回item或者是links 
2.item或者links经过spidermiddleware的process_spider_out()方法，交给engine。 
3.engine将item交给itempipeline,将links交给调度器 
4.在调度器中，先将request对象利用scrapy内置的指纹函数，生成一个指纹对象 
5.如果request对象中的dont_filter参数设置为False,并且该request对象的指纹不在信息指纹的队列中，那么就把该request对象放到优先级的队列中 
6.从优先级队列中获取request对象，交给engine 
7.engine将request对象交给下载器下载，期间会通过downloadmiddleware的process_request()方法 
8.下载器完成下载，获得response对象，将该对象交给engine,期间会通过downloadmiddleware的process_response()方法 
9.engine将获得的response对象交给spider进行解析，期间会经过spidermiddleware的process_spider_input()方法 
10.从第一步开始循环

上面的十个步骤就是scrapy-redis的整体框架，与scrapy相差无几。本质的区别就是，将scrapy的内置的去重的队列和待抓取的request队列换成了redis的集合。就这一个小小的改动，就使得了scrapy-redis支持了分布式抓取。
```

#### 22.urlopen()

```
在 Python3 的 urllib 库中，所有和网络请求的相关方法都被集中到 urllib.request 模块下了。以下是 urlopen() 方法最基本的使用方法：

from urllib import request
resp = request.urlopen('https://www.baidu.com')
print(resp.read())
```

#### 23.urlretrieve()

```
这个方法可以方便的将网页上的一个文件保存到本地。以下代码可以非常方便的将百度的首页下载到本地：

from urllib import request
request.urlretrieve('http://www.baidu.com/', 'baidu.html')
```

#### 24.urlencode()

```
若 ulr 中包含了中文或其他特殊字符，则浏览器会自动给我们进行编码。要用爬虫模拟浏览器，就需要进行手动编码，此时就要用到 urlencode() 方法来实现。这个方法可以把字典数据转换为 url 编码的数据。例如：

from urllib import parse
 
url = 'https://www.baidu.com/s'
 
params = {"wd": "教学设计"}
qs = parse.urlencode(params)
url = url + "?ie=UTF-8&&" + qs
print(qs)
print(url)
resp = request.urlopen(url)
print(resp.read())
```

#### 25.parse_qs()

```
此方法可以将经过编码后的 ulr 参数进行解码。例如：

from urllib import parse
 
params = {'name': '木易', 'age': '21', 'sex': 'male'}
qs = parse.urlencode(params)
result = parse.parse_qs(qs)
print(result)
```

#### 26.urlparse() 和 urlsplit()

```
这两个方法用来对 url 的各个组成部分进行分割。最后可以一次性拿到全部的 url 组成部分，也可以单独拿出某个部分。例如：

from urllib import parse
 
url = 'https://baike.baidu.com/item/hello%20world/85501?fr=aladdin#2_13'
 
result = parse.urlparse(url)
print(result) //拿出全部
// 分别拿出各个部分
print('scheme:', result.scheme)
print('netloc:', result.netloc)
print('path:', result.path)
print('params:',result.params)  // urlsplit() 方法没有此项结果
print('query:', result.query)
print('fragment:', result.fragment)
```

#### 27.写爬虫是用多进程好？还是多线程好？ 为什么？
```
IO密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率(单线程下有IO操作会进行IO等待，造成不必要的时间浪费，而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。在实际的数据采集过程中，既考虑网速和响应的问题，也需要考虑自身机器的硬件情况，来设置多进程或多线程
```

#### 28.解析网页的解析器使用最多的是哪几个
```
1 正则表达式:模糊匹配解析
2 html.parser:结构化解析
3 Beautiful Soup :结构化解析
4 lxml:结构化解析
其中 Beautiful Soup 功能很强大,有html.parse和 lxml的解析器.
结构化解析-DOM(Document Object Model)树
或者：Re、json、jsonpath、BeautifulSoup、pyquery、lxml。
```

#### 29.爬取数据后使用哪个数据库存储数据的，为什么？
```
一般爬虫使用的数据库，是根据项目来定的。如需求方指定了使用什么数据库、如果没指定，那么决定权就在爬虫程序员手里，如果自选的话，mysql 和 mongodb 用的都是比较多的。但不同的数据库品种有各自的优缺点，不同的场景任何一种数据库都可以用来存储，但是某种可能会更好。比如如果抓取的数据之间的耦合性很高，关系比较复杂的话，那么 mysql 可能会是更好的选择。如果抓取的数据是分版块的，并且它们之间没有相似性或关联性不强，那么可能 mongodb 会更好。。另外主流的几种永

久存储数据库，都是具备处理高并发、具备存储大量数据的能力的，只是由于各自的实现机制不一样，

因此优化方案也是不尽相同。总结就是：数据库的选择尽量从项目的数据存在的特性来考虑，还有一个问题就是开发人员最擅长那种数据库。

MongoDB 是使用比较多的数据库，这里以 MongoDB 为例，大家需要结合自己真实开发环境回答。

原因：1）与关系型数据库相比，MongoDB 的优点如下。

①弱一致性（最终一致），更能保证用户的访问速度

举例来说，在传统的关系型数据库中，一个 COUNT 类型的操作会锁定数据集，这样可以保证得到“当前”情况下的较精确值。这在某些情况下，例 如通过 ATM 查看账户信息的时候很重要， 但对于 Wordnik 来说，数据是不断更新和增长的，这种“较精确”的保证几乎没有任何意义，反而会产生很大的延 迟。他们需要的是一个“大约”的数字以及更快的处理速度。

但某些情况下 MongoDB 会锁住数据库。如果此时正有数百个请求，则它们会堆积起来，造成许多问题。我们使用了下面的优化方式来避免锁定。

每次更新前，我们会先查询记录。查询操作会将对象放入内存，于是更新则会尽可能的迅速。在主/ 从部署方案中，从节点可以使用“-pretouch”参数运行，这也可以得到相同的效果。

使用多个 mongod 进程。我们根据访问模式将数据库拆分成多个进程。

②文档结构的存储方式，能够更便捷的获取数据。

对于一个层级式的数据结构来说，如果要将这样的数据使用扁平式的，表状的结构来保存数据， 这无论是在查询还是获取数据时都十分困难。

③ 内 置 GridFS， 支 持 大 容 量 的 存 储 。GridFS 是一个出色的分布式文件系统，可以支持海量的数据存储。内置了 GridFS 了 MongoDB， 能够满足对大数据集的快速范围查询。

④内置 Sharding。

提供基于 Range 的 Auto Sharding 机制：一个 collection 可按照记录的范围，分成若干个段， 切分到不同的 Shard 上。Shards 可以和复制结合，配合 Replica sets 能够实现 Sharding+fail-over， 不同的 Shard 之间可以负载均衡。查询是对 客户端是透明的。客户端执行查询，统计，MapReduce 等操作，这些会被 MongoDB 自动路由到后端的数据节点。这让我们关注于自己的业务，适当的 时候可以无痛的升级。MongoDB 的 Sharding 设计能力较大可支持约 20 petabytes，足以支撑一般应用。

这可以保证 MongoDB 运行在便宜的 PC 服务器集群上。PC 集群扩充起来非常方便并且成本很低， 避免了“sharding”操作的复杂性和成本。

⑤第三方支持丰富。(这是与其他的 NoSQL 相比，MongoDB 也具有的优势)

现在网络上的很多 NoSQL 开源数据库完全属于社区型的，没有官方支持，给使用者带来了很大的风险。而开源文档数据库 MongoDB 背后有商业公司 10gen 为其提供供商业培训和支持。

而且 MongoDB 社区非常活跃，很多开发框架都迅速提供了对 MongDB 的支持。不少知名大公司和网站也在生产环境中使用 MongoDB，越来越多的创新型企业转而使用 MongoDB 作为和

Django，RoR 来搭配的技术方案。

⑥性能优越

在使用场合下，千万级别的文档对象，近 10G 的数据，对有索引的 ID 的查询不会比 mysql 慢， 而对非索引字段的查询，则是全面胜出。 mysql 实际无法胜任大数据量下任意字段的查询，而

mongodb 的查询性能实在让我惊讶。写入性能同样很令人满意，同样写入百万级别的数 据，

mongodb 比我以前试用过的 couchdb 要快得多，基本 10 分钟以下可以解决。补上一句，观察过程中 mongodb 都远算不上是 CPU 杀手。

2)Mongodb 与 redis 相比较

①mongodb 文件存储是 BSON 格式类似 JSON，或自定义的二进制格式。

mongodb 与 redis 性能都很依赖内存的大小，mongodb 有丰富的数据表达、索引；最类似于关系数据库，支持丰富的查询语言，redis 数据丰富，较少的 IO ，这方面 mongodb 优势明显。

②mongodb 不支持事物，靠客户端自身保证，redis 支持事物，比较弱，仅能保证事物中的操作按顺序执行，这方面 redis 优于 mongodb。

③mongodb 对海量数据的访问效率提升，redis 较小数据量的性能及运算,这方面 mongodb 性能优于redis .monbgodb 有mapredurce 功能，提供数据分析，redis 没有 ，这方面 mongodb优 于 redis 。

 
————————————————
版权声明：本文为CSDN博主「limengshi138392」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/limengshi138392/article/details/90734963
```

#### 30.你用过的爬虫框架或者模块有哪些？谈谈他们的区别或者优缺点？
```
Python 自带：urllib、urllib2 第三方：requests，aiohttp 框架： Scrapy、pyspider

urllib 和urllib2 模块都做与请求 URL 相关的操作，但他们提供不同的功能。

urllib2：urllib2.urlopen 可以接受一个 Request 对象或者 url，（在接受 Request 对象时候，并以此可以来设置一个 URL 的 headers），urllib.urlopen 只接收一个 url。

urllib 有urlencode,urllib2 没有，因此总是urllib，urllib2 常会一起使用的原因。

scrapy 是封装起来的框架，它包含了下载器，解析器，日志及异常处理，基于多线程，twisted 的方式处理，对于固定单个网站的爬取开发，有优势，但是对于多网站爬取，并发及分布式处理方面，不够灵活，不便调整与括展。

request 是一个 HTTP 库， 它只是用来，进行请求，对于 HTTP 请求，他是一个强大的库，下载， 解析全部自己处理，灵活性更高，高并发与分布式部署也非常灵活，对于功能可以更好实现

aiohttp 是一个基于 python3 的 asyncio 携程机制实现的一个 http 库，相比 requests，aiohttp 自身就具备了异步功能。但只能在 python3 环境使用。

Scrapy 优点：

scrapy 是异步的；

采取可读性更强的 xpath 代替正则； 强大的统计和 log 系统；

同时在不同的 url 上爬行；

支持 shell 方式，方便独立调试；

写 middleware,方便写一些统一的过滤器； 通过管道的方式存入数据库；

Scrapy 缺点：

基于 python 的爬虫框架，扩展性比较差；

基于 twisted 框架，运行中的 exception 是不会干掉 reactor，并且异步框架出错后是不会停掉其他任务的，数据出错后难以察觉。

Pyspider 是一个重量级的爬虫框架，为什么说重量级？我们知道 scrapy 没有数据库集成、没有分布式、断点续爬的支持、没有 UI 控制界面等等、如果基于 scrapy 想要实现这些功能，都需要自行开发，但 pyspider 都已经集成了。但也正因如此 pyspider 另一个问题就是，扩展性太差，学习难度较大

```

#### 31.网站如何断定该请求是一个爬虫：
```
请求头、请求频率、IP 地址、cookie 等（持续更新请求头、cookie、用户 cookie 池、代理 IP 池、设置一定的延时）
```

#### 32.网站如何制裁这个它认为是爬虫的请求（假数据、空数据、不返回响应、验证码、4xx 状态码等）
```
通过 Headers 反爬虫：
从用户请求的 Headers 反爬虫是最常见的反爬虫策略。很多网站都会对 Headers 的 User-Agent 进行检测，还有一部分网站会对 Referer 进行检测（一些资源网站的防盗链就是检测 Referer）。如果遇到了这类反爬虫机制，可以直接在爬虫中添加 Headers，将浏览器的 User-Agent 复制到爬虫的

Headers 中；或者将 Referer 值修改为目标网站域名。对于检测 Headers 的反爬虫，在爬虫中修改或者添加 Headers 就能很好的绕过。

基于用户行为反爬虫：
还有一部分网站是通过检测用户行为，例如同一 IP 短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。

大多数网站都是前一种情况，对于这种情况，使用 IP 代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理 ip，检测后全部保存起来。这样的代理 ip 爬虫经常会用到，最好自己准备一个。有了大量代理 ip 后可以每请求几次更换一个 ip，这在 requests 或者 urllib2 中很容易做到，这样就能很容易的绕过第一种反爬虫。

对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。

动态页面的反爬虫：
上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过 ajax 请求得到，或者通过 JavaScript 生成的。首先用 Fiddler 对网络请求进行分析。如果能够找到 ajax 请求， 也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用 requests 或者 urllib2 模拟 ajax 请求，对响应的 json 进行分析得到需要的数据。

能够直接模拟ajax 请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。这种情况下就用 selenium+phantomJS，调用浏览器内核，并利用 phantomJS 执行 js 来模拟人为操作以及触发页面中的 js 脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。

用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加

Headers 一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS 就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利 selenium+phantomJS 能干很多事情，例如识别点触式

（12306）或者滑动式的验证码，对页面表单进行暴力破解等。

```

#### 33.使用最多的数据库（Mysql，Mongodb，redis 等），对他们的理解？
```
MySQL 数据库：开源免费的关系型数据库，需要实现创建数据库、数据表和表的字段，表与表之间可以进行关联（一对多、多对多），是持久化存储。

Mongodb 数据库：是非关系型数据库，数据库的三元素是，数据库、集合、文档，可以进行持久化存储，也可作为内存数据库，存储数据不需要事先设定格式，数据以键值对的形式存储。

redis 数据库：非关系型数据库，使用前可以不用设置格式，以键值对的方式保存，文件格式相对自由，主要用与缓存数据库，也可以进行持久化存储。

```

#### 34.字符集和字符编码
```
字符是各种文字和符号的总称，包括各个国家文字、标点符号、图形符号、数字等。

字符集是多个字符的集合，字符集种类较多，每个字符集包含的字符个数不同，常见字符集有：

ASCII 字符集、ISO 8859 字符集、GB2312 字符集、BIG5 字符集、GB18030 字符集、Unicode 字符集等。

字符编码就是以二进制的数字来对应字符集的字符。常见的编码字符集（简称字符集）如下所示：

Unicode：也叫统一字符集，它包含了几乎世界上所有的已经发现且需要使用的字符

（如中文、日文、英文、德文等）。

ASCII：ASCII 既是编码字符集，又是字符编码。早期的计算机系统只能处理英文，所以 ASCII 也就成为了计算机的缺省字符集，包含了英文所需要的所有字符。

GB2312：中文字符集，包含 ASCII 字符集。ASCII 部分用单字节表示，剩余部分用双字节表示。

GBK：GB2312 的扩展，但完整包含了 GB2312 的所有内容。

GB18030：GBK 字符集的超集，常叫大汉字字符集，也叫 CJK（Chinese，Japanese， Korea）字符集，包含了中、日、韩三国语。

注意：Unicode 字符集有多种编码方式，如 UTF-8、UTF-16 等；ASCII 只有一种；大多数 MBCS（包括 GB2312）也只有一种。

```

#### 35.“极验”滑动验证码如何破解？
```
1. selenium 控制鼠标实现，速度太机械化，成功率比较低

2. 计 算 缺 口 的 偏 移 量 （ 推 荐 博 客 ： http://blog.csdn.net/paololiu/article/details/52514504?%3E

3. “极验”滑动验证码需要具体网站具体分析，一般牵扯算法乃至深度学习相关知识。

```

#### 36.爬的那些内容数据量有多大，多久爬一次，爬下来的数据是怎么存储？
```
京东整站的数据大约在 1 亿左右，爬下来的数据存入数据库，mysql 数据库中如果有重复的 url 建议去重存入数据库，可以考虑引用外键。评分，评论如果做增量，Redis 中 url 去重，评分和评论建议建立一张新表用 id 做关联。

多久爬一次这个问题要根据公司的要求去处理，不一定是每天都爬。

Mongo 建立唯一索引键（id）可以做数据重复 前提是数据量不大 2 台电脑几百万的情况 数据库需要做分片 （数据库要设计合理）。

例：租房的网站数据量每天大概是几十万条 ，每周固定爬取。

```

#### 37.动态加载又对及时性要求很高怎么处理？
```
分析出数据接口 API（利用 chrome 或者抓包工具），然后利用 requests 等 http 库实现爬虫，如果有需要执行 js 代码，可借助 js2py、pyexecjs 等工具实现。

Selenium+Phantomjs


```

#### 38.谈一谈你对Selenium 和PhantomJS 了解
```
Selenium 是一个 Web 的自动化测试工具，可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。Selenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行， 所以我们可以用一个叫 PhantomJS 的工具代替真实的浏览器。Selenium 库里有个叫 WebDriver 的

API。WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他

Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫。

PhantomJS 是一个基于 Webkit 的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。相比传统的 Chrome或 Firefox 浏览器等，资源消耗会更少。

如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScript、Cookie、headers，以及任何我们真实用户需要做的事情。

主程序退出后，selenium 不保证 phantomJS 也成功退出，最好手动关闭 phantomJS 进程。（有可能会导致多个 phantomJS 进程运行，占用内存）。

WebDriverWait 虽然可能会减少延时，但是目前存在 bug（各种报错），这种情况可以采用 sleep。

phantomJS 爬数据比较慢，可以选择多线程。如果运行的时候发现有的可以运行，有的不能，可以尝试将 phantomJS 改成 Chrome

```

#### 39.代理 IP 里的“透明”“匿名”“高匿”分别是指？
```
透明代理的意思是客户端根本不需要知道有代理服务器的存在，但是它传送的仍然是真实的 IP。你要想隐藏的话，不要用这个。

普通匿名代理能隐藏客户机的真实 IP，但会改变我们的请求信息，服务器端有可能会认为我们使用了代理。不过使用此种代理时，虽然被访问的网站不能知道你的 ip 地址，但仍然可以知道你在使用代理，当然某些能够侦测 ip 的网页仍然可以查到你的 ip。

高匿名代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真实 IP 是隐藏的，服务器端不会认为我们使用了代理。

设置代理有以下两个好处：

1，让服务器以为不是同一个客户端在请求

2，防止我们的真实地址被泄露，防止被追究

```

#### 40.通用爬虫 ：()通常指搜索引擎的爬虫)
```
聚焦爬虫 ：针对特定网站的爬虫

通用搜素引擎的局限性：

通用搜索引擎所返回的网页里 90%的内容无用。


图片、数据库、音频、视频多媒体的内容通用搜索引擎无能为力。不同用户搜索的目的不全相同，但是返回内容相同。
```


#### 41.微信公众号数据如何抓取?
```
sogou 微信搜索数据
```

#### 42.股票数据的获取目前有如下两种方法可以获取:
```
http/JavaScript 接口取数据

　　web-service 接口

　　Sina 股票数据接口

　　以大秦铁路(股票代码：601006)为例，如果要获取它的最新行情，只需访问新浪的股票数据，只需访问新浪的股票数据接口：http://hq.sinajs.cn/list=sh具体股票代码编号
```

#### 43.分布式有哪些方案，哪一种最好?
```
celery、beanstalk，gearman
个人认为 gearman 比较好。原因主要有以下几点：

　　技术类型简单，维护成本低。

　　简单至上。能满足当前的技术需求即可 (分布式任务处理、异步同步任务同时支持、任务队列的持久化、维

　　有成熟的使用案例。instagram 就是使用的 gearman来完成图片的处理的相关任务，有成功的经验，我们当然应该借鉴。
```

#### 44.简述一下爬虫程序执行的流程
```
获取想要的页面

　　根据规则进行解析

　　解析数据入库
```

#### 45.爬虫在向数据库存数据开始和结束都会发一条消息，是scrapy 哪个模块实现的?
```
Item Pipeline scrapy 的信号处理使用的是
```

#### 46.爬取下来的数据如何去重，说一下具体的算法依据。
```
通过 MD5 生成电子指纹来判断页面是否改变

　　nutch 去重。nutch 中 digest 是对采集的每一个网页内容的 32 位哈希值，如果两个网页内容完全一样，它们的 digest值肯定会一样。
```

#### 47.mysql的索引在什么情况下失效
```
1.如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)
要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
2.对于多列索引，不是使用的第一部分，则不会使用索引
3.like查询以%开头
4.如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
5.如果mysql估计使用全表扫描要比使用索引快,则不使用索引
```

#### 48. MySQL 有什么引擎，各引擎之间有什么区别？
```
主要 MyISAM 与 InnoDB 两个引擎，其主要区别如下：
1、InnoDB 支持事务，MyISAM 不支持，这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而 MyISAM就不可以了；
2、MyISAM 适合查询以及插入为主的应用，InnoDB 适合频繁修改以及涉及到安全性较高的应用；
3、InnoDB 支持外键，MyISAM 不支持；
4、MyISAM 是默认引擎，InnoDB 需要指定；
5、InnoDB 不支持 FULLTEXT 类型的索引；
6、InnoDB 中不保存表的行数，如 select count() from table 时，InnoDB；需要扫描一遍整个表来计算有多少行，但是 MyISAM 只要简单的读出保存好的行数即可。注意的是，当 count()语句包含 where 条件时 MyISAM 也需要扫描整个表；
7、对于自增长的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM表中可以和其他字段一起建立联合索引；
8、清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表；
9、InnoDB 支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like '%lee%'
```

#### 49.HTTPS 是如何实现安全传输数据的
```
客户端（通常是浏览器）先向服务器发出加密通信的请求
服务器收到请求,然后响应
客户端收到证书之后会首先会进行验证
服务器收到使用公钥加密的内容，在服务器端使用私钥解密之后获得随机数pre-master secret，然后根据radom1、radom2、pre-master secret通过一定的算法得出session Key和MAC算法秘钥，作为后面交互过程中使用对称秘钥。同时客户端也会使用radom1、radom2、pre-master secret，和同样的算法生成session Key和MAC算法的秘钥。
然后再后续的交互中就使用session Key和MAC算法的秘钥对传输的内容进行加密和解密。
```

#### 50.描述下scrapy 框架运行的机制？
```
从start_urls里获取第一批url并发送请求，请求由引擎交给调度器入请求队列，获取完毕后，调度器将请求队列里的请求交给下载器去获取请求对应的响应资源，并将响应交给自己编写的解析方法做提取处理：
如果提取出需要的数据，则交给管道文件处理；
如果提取出url，则继续执行之前的步骤（发送url请求，并由引擎将请求交给调度器入队列...)，直到请求队列里没有请求，程序结束。
```

#### 51.数据结构之堆，栈和队列的理解和实现。
```
栈（stacks）：栈的特点是后进先出。只能通过访问一端来实现数据的储存和检索的线性数据结构。

队列（queue）：队列的特点是先进先出。元素的增加只能在一端，元素的删除只能在另一端。增加的一端称为队尾，删除的一端称为队首。

栈：

复制代码
1 stack = [1, 2, 3]
2 stack.append(4)
3 stack.append(5)
4 print(stack)
5 stack.pop()
6 stack.pop()
7 print(stack)
复制代码
输出结果：

1 [1, 2, 3, 4, 5]
2 [1, 2, 3]
队列：

复制代码
1 from collections import deque
2 
3 queue = deque(['Eric', 'John', 'Michael'])
4 queue.append('Terry')
5 queue.append('Graham')
6 print(queue)
7 queue.popleft()
8 print(queue)
复制代码
输出结果：

1 deque(['Eric', 'John', 'Michael', 'Terry', 'Graham'])
2 deque(['John', 'Michael', 'Terry', 'Graham'])
```

#### 52.什么是栈溢出？
```
因为栈一般默认为1-2m，一旦出现死循环或者是大量的递归调用，在不断的压栈过程中，造成栈容量超过1m而导致溢出。

栈溢出的几种情况？

1、局部数组过大。当函数内部数组过大时，有可能导致堆栈溢出。

2、递归调用层次太多。递归函数在运行时会执行压栈操作，当压栈次数太多时，也会导致堆栈溢出。

 解决方法：

1、用栈把递归转换成非递归。

2、增大栈空间。
```